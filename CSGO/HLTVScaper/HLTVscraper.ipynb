{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "2fb26068bd0d1e0f8e0cb0471b51822d32bcef174b6ed9ec0f79e5a512585ab8"
   }
  },
  "interpreter": {
   "hash": "75a88bca18d42798bd43853615f6d8e1ba2122f998d1455de99fcbfb11fe8fcf"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "from selenium import webdriver\r\n",
    "from bs4 import BeautifulSoup\r\n",
    "import pandas as pd\r\n",
    "import random\r\n",
    "from datetime import datetime\r\n",
    "import time\r\n",
    "import re\r\n",
    "import json\r\n",
    "import os.path\r\n",
    "from os import path"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "driver = webdriver.Chrome(executable_path='C:\\chromedriver\\chromedriver.exe')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Get number of matches from HLTV given full content and 1 star fitler\r\n",
    "# driver.get('https://www.hltv.org/results?content=stats&stars=1')\r\n",
    "driver.get('https://www.hltv.org/results?content=stats')\r\n",
    "content = driver.page_source\r\n",
    "soup = BeautifulSoup(content)\r\n",
    "n = int(soup.find('span', {'class':'pagination-data'}).text.split()[-1])\r\n",
    "offset = range(0, n, 100)\r\n",
    "print(n)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "55315\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if not path.exists('matchIDs.csv'):\r\n",
    "\r\n",
    "    matchIDs = pd.DataFrame(columns=['Date', 'ID', 'Status'])\r\n",
    "\r\n",
    "    for page in offset:\r\n",
    "\r\n",
    "        driver.get('https://www.hltv.org/results?offset=' + str(page) + '&content=stats')\r\n",
    "        content = driver.page_source\r\n",
    "        soup = BeautifulSoup(content)\r\n",
    "        contentpage = soup.find('div', {'class':'results-holder allres'})\r\n",
    "\r\n",
    "        for subset in contentpage.findAll('div', {'class':'results-sublist'}):\r\n",
    "            date = subset.find('div', {'class':'standard-headline'}).text\r\n",
    "            date = date[12:]\r\n",
    "\r\n",
    "            for URL in subset.findAll('a', {'class':'a-reset'}):\r\n",
    "                ID = URL['href'].split('/')[2]\r\n",
    "                matchIDs = matchIDs.append({'Date':date, 'ID':int(ID), 'Status':0}, ignore_index = True)\r\n",
    "\r\n",
    "        print(page)\r\n",
    "        sleeptime = random.uniform(1, 2)\r\n",
    "        time.sleep(sleeptime)\r\n",
    "\r\n",
    "    matchIDs = matchIDs[::-1].reset_index(drop = True)\r\n",
    "    matchIDs.to_csv('matchIDs.csv', index = False)\r\n",
    "\r\n",
    "else:\r\n",
    "    print('File already exists')\r\n",
    "    matchIDs = pd.read_csv('matchIDs.csv')"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "if path.exists('matchIDs.csv'):\r\n",
    "\r\n",
    "    harvest = pd.DataFrame()\r\n",
    "\r\n",
    "    for page in offset:\r\n",
    "        driver.get('https://www.hltv.org/results?offset=' + str(page) + '&content=stats&stars=1')\r\n",
    "        content = driver.page_source\r\n",
    "        soup = BeautifulSoup(content)\r\n",
    "        contentpage = soup.find('div', {'class':'results-holder allres'})\r\n",
    "        batch = pd.DataFrame(columns=['Date', 'ID', 'Status'])\r\n",
    "\r\n",
    "        for subset in contentpage.findAll('div', {'class':'results-sublist'}):\r\n",
    "            date = subset.find('div', {'class':'standard-headline'}).text\r\n",
    "            date = date[12:]\r\n",
    "\r\n",
    "            for URL in subset.findAll('a', {'class':'a-reset'}):\r\n",
    "                ID = URL['href'].split('/')[2]\r\n",
    "                batch = batch.append({'Date':date, 'ID':int(ID), 'Status':0}, ignore_index = True)\r\n",
    "        \r\n",
    "        booler = batch['ID'].isin(matchIDs['ID'])\r\n",
    "        batch = batch[~booler]\r\n",
    "        if sum(booler) == 100:\r\n",
    "            print('Break')\r\n",
    "            break\r\n",
    "\r\n",
    "        harvest = harvest.append(batch)\r\n",
    "        print(\" page\" + str(page))\r\n",
    "        # sleeptime = random.uniform(1, 2)\r\n",
    "        # time.sleep(sleeptime)\r\n",
    "\r\n",
    "    harvest = harvest[::-1]\r\n",
    "    matchIDs = matchIDs.append(harvest).reset_index(drop = True)\r\n",
    "    print(str(len(harvest)) + ' new matche(s) has/have been found and added')\r\n",
    "\r\n",
    "else:\r\n",
    "    print('File does not exist, initialize first')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Break\n",
      "Elapsed: 0.8620092868804932 page0\n",
      "0 new matche(s) has/have been found and added\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "from     __future__ import print_function\r\n",
    "import sys\r\n",
    "import time\r\n",
    "from etaprogress.progress import ProgressBar\r\n",
    "from IPython.display import clear_output\r\n",
    "\r\n",
    "total = 5\r\n",
    "bar = ProgressBar(total, max_width=400)\r\n",
    "for i in range(total + 1):\r\n",
    "    bar.numerator = i\r\n",
    "    print(bar)\r\n",
    "    clear_output(wait=True)\r\n",
    "    time.sleep(1)\r\n",
    "print()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "iterations = 0\r\n",
    "maxiter = 10000\r\n",
    "bar = ProgressBar(maxiter, max_width=400)\r\n",
    "\r\n",
    "data = {}\r\n",
    "initstart = time.time()\r\n",
    "\r\n",
    "for row in matchIDs.itertuples():\r\n",
    "\r\n",
    "    if row.Status == 1 or row.Status == -1:\r\n",
    "        continue\r\n",
    "\r\n",
    "    try:\r\n",
    "        driver.get('https://www.hltv.org/matches/' + str(row.ID) +'/anything')\r\n",
    "        content = driver.page_source\r\n",
    "        soup = BeautifulSoup(content)\r\n",
    "\r\n",
    "        matchdata  = {}\r\n",
    "        matchdata['Date'] = row.Date\r\n",
    "        matchdata['Event'] = soup.find('div', {'class':'event text-ellipsis'}).text\r\n",
    "\r\n",
    "        team1info = soup.find('div', {'class': 'team1-gradient'})\r\n",
    "        team1name = team1info.find('div', {'class':'teamName'}).text\r\n",
    "        team1score = team1info.find('div', {'class':['won', 'lost', 'tie']}).text\r\n",
    "\r\n",
    "        team2info = soup.find('div', {'class': 'team2-gradient'})\r\n",
    "        team2name = team2info.find('div', {'class':'teamName'}).text\r\n",
    "        team2score = team2info.find('div', {'class':['won', 'lost', 'tie']}).text\r\n",
    "\r\n",
    "        teams = [team1name, team2name]\r\n",
    "        score = [team1score, team2score]\r\n",
    "        matchdata['Teams'] = teams\r\n",
    "        matchdata['Score'] = score\r\n",
    "\r\n",
    "        maps = []\r\n",
    "        mapinfo = soup.find('div', {'class':'flexbox nowrap'})\r\n",
    "        for m in mapinfo.findAll('div', {'class':'dynamic-map-name-full'})[1:]:\r\n",
    "            maps.append(m.text)\r\n",
    "        matchdata['Maps'] = maps\r\n",
    "\r\n",
    "        try:\r\n",
    "            oddsinfo = soup.find('div', {'class':'past-matches-grid'})\r\n",
    "            odds = oddsinfo.findAll('div', {'class':'past-matches-bottom-right-numbers'})\r\n",
    "            odds = [odds[0].text, odds[1].text]\r\n",
    "        except:\r\n",
    "            odds = 'No odds available'\r\n",
    "        \r\n",
    "        matchdata['Odds'] = odds\r\n",
    "\r\n",
    "        matchlinks = []\r\n",
    "        for div in soup.findAll('div', {'class':'results-center-stats'}):\r\n",
    "            for a in div.findAll('a'):\r\n",
    "                link = a['href']\r\n",
    "                matchlinks.append(link)\r\n",
    "\r\n",
    "        match = {}\r\n",
    "        count = 0\r\n",
    "        for link in matchlinks:\r\n",
    "\r\n",
    "            driver.get('https://www.hltv.org/' + str(link))\r\n",
    "            content = driver.page_source\r\n",
    "            soup = BeautifulSoup(content)\r\n",
    "\r\n",
    "            game_stats = {}\r\n",
    "            scoreboard = soup.findAll('table', {'class':'stats-table'})\r\n",
    "            team1 = pd.read_html(str(scoreboard))[0].values.tolist()\r\n",
    "            team2 = pd.read_html(str(scoreboard))[1].values.tolist()\r\n",
    "            scoreboard = {}\r\n",
    "            scoreboard['Team 1'] = team1\r\n",
    "            scoreboard['Team 2'] = team2\r\n",
    "            game_stats['Scoreboard'] = scoreboard\r\n",
    "\r\n",
    "            misc_stats = soup.findAll('div', attrs={'class': 'right'})\r\n",
    "            game_stats['Starting side'] = misc_stats[0].contents[4]['class'][0][:-6]\r\n",
    "            game_stats['Rounds'] = misc_stats[0].text\r\n",
    "            game_stats['Team rating'] = misc_stats[1].text\r\n",
    "            game_stats['Entries'] = misc_stats[2].text\r\n",
    "            game_stats['Clutches'] = misc_stats[3].text\r\n",
    "\r\n",
    "            count += 1\r\n",
    "            match['Game ' + str(count)] = game_stats\r\n",
    "\r\n",
    "            # sleeptime = random.uniform(0, 1)\r\n",
    "            # time.sleep(sleeptime)\r\n",
    "\r\n",
    "        matchdata['Match'] = match\r\n",
    "        data[str(row.ID)] = matchdata\r\n",
    "        matchIDs.loc[row.Index, 'Status'] = 1\r\n",
    "\r\n",
    "    except:\r\n",
    "        matchIDs.loc[row.Index, 'Status'] = -1\r\n",
    "\r\n",
    "    iterations +=1\r\n",
    "\r\n",
    "    bar.numerator = iterations\r\n",
    "    print(bar)\r\n",
    "    clear_output(wait=True)\r\n",
    "\r\n",
    "\r\n",
    "    # print(str(iterations) + ' out of ' + str(maxiter))\r\n",
    "\r\n",
    "    if iterations == maxiter:\r\n",
    "        break\r\n",
    "\r\n",
    "print('Finished')\r\n",
    "\r\n",
    "if path.exists('main.json'):\r\n",
    "    with open('main.json', 'r+') as f:\r\n",
    "        main = json.load(f)\r\n",
    "        main.update(data)\r\n",
    "        f.seek(0)\r\n",
    "        json.dump(main, f, indent = 1)\r\n",
    "        f.close()\r\n",
    "else:\r\n",
    "    j = json.dumps(data, indent = 1)\r\n",
    "    with open('main.json', 'w') as f:\r\n",
    "        f.write(j)\r\n",
    "        f.close()\r\n",
    "\r\n",
    "matchIDs.to_csv('matchIDs.csv', index = False)\r\n",
    "\r\n",
    "print(str(len(data)) + ' entries have been added')\r\n",
    "print(str(sum(matchIDs.Status == 0)) + ' entries awaiting to be scraped')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Finished\n",
      "9909 entries have been added\n",
      "43015 entries awaiting to be scraped\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "print(str(len(data)) + ' entries have been added')\r\n",
    "print(str(sum(matchIDs.Status == 0)) + ' entries awaiting to be scraped')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "9909 entries have been added\n",
      "43015 entries awaiting to be scraped\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "source": [
    "# Validate\r\n",
    "test = json.load(open('main.json'))\r\n",
    "print(len(test) == sum(matchIDs.Status == 1))\r\n",
    "print(len(test))\r\n",
    "print(sum(matchIDs.Status == 1))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\n",
      "10756\n",
      "10756\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "source": [
    "# Botched\r\n",
    "sum(matchIDs.Status == -1)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "metadata": {},
     "execution_count": 364
    }
   ],
   "metadata": {}
  }
 ]
}